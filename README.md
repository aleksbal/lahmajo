# lahmajo
A sort of gateway/API for local LLama model (deployed as Ollama) to RAG the prompts providing the context suited for particular use case, cache results, etc.

Ollama shall be up and running on a local machine in order to the service to work properly.
