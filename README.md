Reverse proxy for a locally deployed LLama model (running via Ollama) + Retrieval-Augmented Generation (RAG). The service enhances prompts by retrieving context tailored to specific use cases, improving the relevance of the responses. Additionally, it includes result caching to optimize performance and minimize redundant processing.

To function correctly, Ollama must be running on the local machine where this service is deployed.
