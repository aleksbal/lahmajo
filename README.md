Reverse proxy for access to locally deployed LLM models running via Ollama. The service can be used to enhance prompts by retrieving context tailored to specific use cases, improving the relevance of the responses by using Retrieval-Augmented Generation (RAG). Additionally, it can add result caching to optimize performance and minimize redundant processing.

To function correctly, Ollama must be running on the machine where this service is deployed.
